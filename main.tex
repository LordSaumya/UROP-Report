\documentclass[fleqn,10pt]{olplainarticle}
% Use option lineno for line numbers 

\usepackage{bibentry}
\nobibliography*

\title{Data-Driven and Physics-Inspired Machine Learning\\[1ex] \large UROP Report U081420}

\author{Saumya Shah}

\keywords{TK}

\begin{abstract}
    TK
\end{abstract}

\begin{document}
\maketitle

\flushbottom

\thispagestyle{empty}

\tableofcontents

\section{Introduction}
TK

\section{Background}
TK

\section{Related Work}

\begin{itemize}
    \item \bibentry{udrescu2020}
          \begin{itemize}
              \item Existing solutions use genetic algorithms or sparse regression.
              \item This paper uses NNs to find simplifying factors like symmetry or separability in the dataset.
              \item 6 simplifying assumptions/properties are used:
                    \begin{itemize}
                        \item \emph{Units:} variables have known physical units
                        \item \emph{Low-order polynomials:} $f$ is/composed of low-order polynomial(s)
                        \item \emph{Composition:} $f$ is composed of a small set of elementary functions
                        \item \emph{Smoothness:} $f$ is continuous
                        \item \emph{Symmetry:} some variables in $f$ are symmetric
                        \item \emph{Separability}: $f$ can be separated into sum/product of variables
                    \end{itemize}
              \item Full algorithm (recursive in nature):
                    \begin{enumerate}
                        \item \emph{Dimensional analysis:} Reduces the number of dimensions and simplifies data  (makes the model depend on as few variables as possible).
                        \item \emph{Polynomial fit:} Polynomial coefficients are calculated by solving a system of linear equations and testing if the RMSE is lower than the threshold.
                        \item \emph{Brute force:} Expressions of increasing complexity are generated by brute force and tested till the error drops below a certain threshold.
                        \item \emph{NN-based transformations:} Tests for translational symmetry, separability, equality of variables, and other transformations like power, log, etcetera
                    \end{enumerate}
              \item The key improvement of AI Feynman over Eureqa is its ability to decompose the problem into simpler sub-problems with fewer variables.
          \end{itemize}

    \item \bibentry{Khoo2023.2}
          \begin{itemize}
              \item Physics-inspired symbolic regression methods like AI Feynman leverage properties of $f$ like symmetry and separability.
              \item Four variations of AI Feynman were compared
                    \begin{itemize}
                        \item No additional bias
                        \item Observational bias (replacing angular values with their sines/cosines)
                        \item Inductive bias (search space restriction)
                        \item Both observational and inductive bias
                    \end{itemize}
              \item For experiments 1 and 3, none of the equations on the Pareto frontier matched the orbital equation for Mars.
              \item For experiments 2 and 4, 3 out of 9 equations matched.
              \item Experiment 4 (combining inductive and observational biases) was best suited to rediscover the orbital equation of Mars.
          \end{itemize}

    \item \bibentry{Khoo2023.1}
          \begin{itemize}
              \item This paper extends AI Feynman to discover heliocentricity and planarity of Mars' orbit by adding biases.
              \item Experimental Setup
                    \begin{itemize}
                        \item An inductive bias is built in by restricting the search space to trigonometric, polynomial, and radical functions.
                        \item An observational bias is embedded by replacing angular values with their sine and cosine.
                        \item The description length serves as a measure for fit and parsimony.
                        \item There is a log-scaled penalty on absolute loss (for optimising fit) and on real numbers, operators, and variables (for optimising parsimony)
                        \item Experiment 1
                              \begin{itemize}
                                  \item Three sets of observations, corresponding to the three reference frames are created for both coordinate systems (Cartesian and polar) and are used as inputs to AI Feynman.
                                  \item For the Cartesian coordinates, several equations were identified that matched a known equation.
                                  \item For the polar coordinates, none of them matched any known equations. However, one of the equations uses a similar attempt (using angular width) to a known equation.
                                  \item Both coordinate systems preferred heliocentric equations, suggesting a higher parsimony in that reference frame.
                              \end{itemize}
                        \item Experiment 2
                            \begin {itemize}
                                \item Principal component analysis is first used to project the data into 3d, 2d, and 1d spaces, which are then used as inputs to AI Feynman.
                                \item None of the equations match the known equation forms. However, two of the equation use a square root to fit $r(t)$ similar to a known equation.
                                \item Most equations only use one to two eigenvectors suggesting a planar relationship.
                            \end{itemize}
                        \item Experiment 3
                            \begin{itemize}
                                \item Knowledge regarding the heliocentricity and planarity of Mars' orbit are embedded as observational biases.
                                \item Some of the equations suggest a circular orbit due to low eccentricty.
                                \item After correcting for a vertical shift of focus, Kepler's first law was obtained from both the Cartesian and polar datasets. 
                            \end{itemize}
                    \end{itemize}
          \end{itemize}
\end{itemize}

\section{Methodology}
TK

\section{Performance Evaluation}
TK

\section{Conclusion}
TK

\nocite{*}
\bibliography{sample}

\section*{Appendices}
TK

\end{document}